# explore-NLP
This repository serves as an essential guide to the field of Natural Language Processing (NLP), offering a curated list of both foundational and recent influential papers. It is divided into two main sections: the first contains the seminal papers that have shaped the foundation of NLP, and the second provides a chronological list of key papers that reflect the latest advancements and trends in the field. Whether you're a student, a researcher, or an enthusiast, this collection is designed to keep you up-to-date and deepen your understanding of the ever-evolving landscape of NLP.

## Foundational Papers

* **[2013] Distributed Representations of Words and Phrases and their Compositionality** [\[PDF\]](https://arxiv.org/pdf/1310.4546.pdf)
* **[2013] Efficient Estimation of Word Representations in Vector Space** [\[PDF\]](https://arxiv.org/pdf/1301.3781.pdf)
* **[2014] GloVe: Global Vectors for Word Representation** [\[PDF\]](https://aclanthology.org/D14-1162.pdf)
* **[2015] Convolutional Neural Networks for Sentence Classification** [\[PDF\]](https://arxiv.org/pdf/1408.5882.pdf)
* **[2016] Enriching Word Vectors with Subword Information** [\[PDF\]](https://arxiv.org/abs/1607.04606)
* **[2016] Globally Normalized Transition-Based Neural Networks** [\[PDF\]](https://arxiv.org/pdf/1603.06042.pdf)
* **[2017] Attention is all you need** [\[PDF\]](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
* **[2018] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** [\[PDF\]](https://arxiv.org/pdf/1810.04805.pdf)
* **[2018] Deep Contextualized Word Representation (Elmo)** [\[PDF\]](https://arxiv.org/abs/1802.05365)
* **[2018] Improving Language Understanding by Generative Pre-Training(GPT)** [\[PDF\]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
* **[2018] Universal Language Model Fine-tuning for Text Classification (ULMFit)** [\[PDF\]](https://arxiv.org/abs/1801.06146)
* **[2019] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context** [\[PDF\]](https://arxiv.org/pdf/1901.02860v3.pdf)
* **[2019] Generating Long Sequences with Sparse Transformers** [\[PDF\]](https://arxiv.org/abs/1904.10509)
* **[2020] Reformer: The Efficient Transformer** [\[PDF\]](https://arxiv.org/abs/2001.04451)
* **[2020] Language Models are Few-Shot Learners (GPT3)** [\[PDF\]](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)

## Recent papers

### 2021
* **Learning Transferable Visual Models From Natural Language Supervision**  [\[PDF\]](https://arxiv.org/pdf/2103.00020.pdf)
* **Improved Denoising Diffusion Probabilistic Models** [\[PDF\]](https://arxiv.org/pdf/2102.09672.pdf)
* **Diffusion Models Beat GANs on Image Synthesis** [\[PDF\]](https://arxiv.org/pdf/2105.05233.pdf)
* **Image Super-Resolution via Iterative Refinement** [\[PDF\]](https://arxiv.org/pdf/2104.07636.pdf)

### 2022
* **LaMDA: Language Models for Dialog Applications** [\[PDF\]](https://arxiv.org/pdf/2201.08239.pdf)
* **Training language models to follow instructions with human feedback (RLHF)** [\[PDF\]](https://arxiv.org/pdf/2203.02155.pdf)
* **Training Compute-Optimal Large Language Models** [\[PDF\]](https://arxiv.org/pdf/2203.15556.pdf)
* **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** [\[PDF\]](https://arxiv.org/pdf/2201.11903.pdf)
* **PaLM: Scaling Language Modeling with Pathways** [\[PDF\]](https://arxiv.org/pdf/2204.02311.pdf)
* **Competition-Level Code Generation with AlphaCode** [\[PDF\]](https://arxiv.org/pdf/2203.07814.pdf)

### 2023
* **Scaling Laws for Generative Mixed-Modal Language Models** [\[PDF\]](https://arxiv.org/pdf/2301.03728.pdf)
* **Multimodal Chain-of-Thought Reasoning in Language Models** [\[PDF\]](https://arxiv.org/pdf/2302.00923.pdf)
* **LLaMA: Open and Efficient Foundation Language Models** [\[PDF\]](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)
* **PaLM-E: An Embodied Multimodal Language Model** [\[PDF\]](https://arxiv.org/pdf/2303.03378.pdf)
* **GPT-4 Technical Report** [\[PDF\]] (https://arxiv.org/pdf/2303.08774v2.pdf)
* **LERF: Language Embedded Radiance Fields** [\[PDF\]] (https://arxiv.org/pdf/2303.09553.pdf)
