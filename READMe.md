# explore-NLP
This repository serves as an essential guide to the field of Natural Language Processing (NLP), offering a curated list of both foundational and recent influential papers. It is divided into two main sections: the first contains the seminal papers that have shaped the foundation of NLP, and the second provides a chronological list of key papers that reflect the latest advancements and trends in the field. Whether you're a student, a researcher, or an enthusiast, this collection is designed to keep you up-to-date and deepen your understanding of the ever-evolving landscape of NLP.

## Foundational Papers

### 2013
* **Distributed Representations of Words and Phrases and their Compositionality** [\[PDF\]](https://arxiv.org/pdf/1310.4546.pdf)
* **Efficient Estimation of Word Representations in Vector Space** [\[PDF\]](https://arxiv.org/pdf/1301.3781.pdf)

### 2014
* **GloVe: Global Vectors for Word Representation** [\[PDF\]](https://aclanthology.org/D14-1162.pdf)

### 2015
* **Convolutional Neural Networks for Sentence Classification** [\[PDF\]](https://arxiv.org/pdf/1408.5882.pdf)

### 2016
* **Enriching Word Vectors with Subword Information** [\[PDF\]](https://arxiv.org/abs/1607.04606)
* **Globally Normalized Transition-Based Neural Networks** [\[PDF\]](https://arxiv.org/pdf/1603.06042.pdf)

### 2017
* **Attention is all you need** [\[PDF\]](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

### 2018
* **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** [\[PDF\]](https://arxiv.org/pdf/1810.04805.pdf)
* **Deep Contextualized Word Representation (Elmo)** [\[PDF\]](https://arxiv.org/abs/1802.05365)
* **Improving Language Understanding by Generative Pre-Training(GPT)** [\[PDF\]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
* **Universal Language Model Fine-tuning for Text Classification (ULMFit)** [\[PDF\]](https://arxiv.org/abs/1801.06146)

### 2019
* **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context** [\[PDF\]](https://arxiv.org/pdf/1901.02860v3.pdf)
* **Generating Long Sequences with Sparse Transformers** [\[PDF\]](https://arxiv.org/abs/1904.10509)

### 2020
* **Reformer: The Efficient Transformer** [\[PDF\]](https://arxiv.org/abs/2001.04451)


## Recent papers

### 2021
* **Learning Transferable Visual Models From Natural Language Supervision**  [\[PDF\]](https://arxiv.org/pdf/2103.00020.pdf)
* **Improved Denoising Diffusion Probabilistic Models** [\[PDF\]](https://arxiv.org/pdf/2102.09672.pdf)
* **Diffusion Models Beat GANs on Image Synthesis** [\[PDF\]](https://arxiv.org/pdf/2105.05233.pdf)
* **Image Super-Resolution via Iterative Refinement** [\[PDF\]](https://arxiv.org/pdf/2104.07636.pdf)



